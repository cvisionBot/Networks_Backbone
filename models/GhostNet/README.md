# GhostNet: More Features from Cheep Operations

> **Abstract**
> 
- Convolutional Neural Networks (CNNs) Embedded Deviceì— ë°°ì¹˜í•˜ëŠ” ê²ƒì€ `Limited Memory`ì™€ `Computation Resource` í™˜ê²½ ë•Œë¬¸ì— ì–´ë µë‹¤.
- ë…¼ë¬¸ì—ì„œ ê°„ë‹¨í•œ ì—°ì‚°ìœ¼ë¡œ ë§ì€ Feature-map ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” **Ghost Module** ì œì•ˆí•œë‹¤. Intrinstic Feature-mapì˜ ì§‘í•©ì„ ê¸°ë°˜ìœ¼ë¡œ, ì €ìëŠ” ê°„ë‹¨í•œ ì—°ì‚°ì„ Linear Transformation ì—°ì†ì— ì ìš©í•˜ì—¬ ë§ì€ Ghost Feature-map ë§Œë“¤ì–´ ë‚¸ë‹¤. ì´ Feature-mapì€ ë³¸ì§ˆì ì¸ íŠ¹ì§•ì˜ ê¸°ì´ˆê°€ ë˜ëŠ” ì •ë³´ë¥¼ ë“œëŸ¬ë‚¼ ìˆ˜ ìˆë‹¤.
- ì–¸ê¸‰í•œ Ghost Moduleì€ `plug-and-play` ìš”ì†Œë¡œ ì¥ì°©ë˜ì–´, ê¸°ì¡´ Convolutional Neural Network ë°œì „ ì‹œí‚¬ ìˆ˜ ìˆë‹¤.
- Ghost Module ìŒ“ì•„ì„œ Ghost bottleneck ì„¤ê³„í•˜ê³ , ì´ë¥¼ í†µí•´ ê°€ë²¼ìš´ **GhostNet** ë§Œë“¤ ìˆ˜ ìˆë‹¤.

> **Introduction**
> 

![Figure1](./src/1.jpg)

<aside>
ğŸ“Œ [Knowledge Distillation]
ë¯¸ë¦¬ ì˜ í•™ìŠµëœ í° ë„¤íŠ¸ì›Œí¬(Teacher Network)ì˜ ì§€ì‹ì„ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ì‘ì€ ë„¤íŠ¸ì›Œí¬(Student Network)ì—ê²Œ ì „ë‹¬í•˜ëŠ” ê²ƒ.
- Computing Resource
- Energy
- Memory

</aside>

- ì „í†µì ì¸ CNNsì€ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ë§ì€ Parameterì™€ Floating point operations (FLOPs)ê°€ í•„ìš”í•˜ë‹¤. ë”°ë¼ì„œ ìµœê·¼ Deep Neural Network ì„¤ê³„ì— ê´€í•œ ê²½í–¥ì€ Mobile deviceì— ìˆ˜í–‰ ê°€ëŠ¥í•˜ë„ë¡ íš¨ìœ¨ì ì¸ Network êµ¬ì¡°ë¥¼ ì—°êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.
- "Han"ì€ Nueral Networkì—ì„œ ì¤‘ìš”í•˜ì§€ ì•Šì€ Wegiht ê°€ì§€ë¥¼ ì¹˜ëŠ” ê²ƒì„ ì œì•ˆí–ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë°©ë²•ë“¤ì˜ ìˆ˜í–‰ ëŠ¥ë ¥ì€ ê·¸ë“¤ì´ baselines ì¡ì€ pre-trained Networkì— ê¸°ë°˜í•œë‹¤.
- ì˜ í•™ìŠµëœ Networkì˜ í’ë¶€í•œ Feature-map ì •ë³´ëŠ” Input Data ì´í•´ë¥¼ ë³´ì¥í•œë‹¤. í•˜ì§€ë§Œ **Figure 1**ì²˜ëŸ¼ ì¤‘ë³µëœ Feature-map (Ghost) ë³´ëŠ” ìƒí™©ì´ ë°œìƒí•œë‹¤. ì´ëŸ° í’ë¶€í•œ Feature-map í”¼í•˜ëŠ” ê²ƒ ëŒ€ì‹ ì— ì €ë ´í•˜ê²Œ ì´ê²ƒë“¤ì„ ë³´ê°•í•œë‹¤.
- Output Feature-map í¬ê¸° ë³€ê²½ ì—†ì´, Ghost Mudleì—ì„  í•„ìš”í•œ Parameter ìˆ˜ì™€ ì—°ì‚° ë³µì¡ë„ê°€ ê°ì†Œëœë‹¤. ì´ëŸ¬í•œ ëª¨ë“ˆì— ì…ê°í•˜ì—¬ íš¨ìœ¨ì ì¸ êµ¬ì¡°ì¸ **GhostNet** ì œì‘í•˜ì˜€ë‹¤.

> **Approach**
> 

ì €ìë“¤ì€ ì ì€ Fileter ì‚¬ìš©í•˜ì—¬ ë§ì€ Feature-map ë§Œë“¤ì–´ë‚´ëŠ” Ghost Module ì²˜ìŒ ë„ì…í•˜ì˜€ë‹¤. ê·¸ë¦¬ê³  GhostNet ë°œì „ ì‹œì¼°ê³ , ë§¤ìš° íš¨ìœ¨ì ì´ë©° ë†’ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤.

**1. Ghost Module for More Features**

![Figure2](./src/2.jpg)

- Deep Convolutional Neural Networks ë‹¤ìˆ˜ì˜ Convolution êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ë§ì€ ì—°ì‚°ì„ ë™ë°˜í•œë‹¤. ë¹„ë¡ ìµœì‹  ì—°êµ¬ê°€ CNNsì˜ ì‘ì€ Conv Filter ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ êµ¬ì¡°ë¥¼ ë§Œë“¤ë ¤ê³  í•˜ì§€ë§Œ ì—¬ì „íˆ 1 x 1 Convolutionì€ ìƒë‹¹í•œ `Memory`ì™€ `FLOPs` ì°¨ì§€í•œë‹¤.
- Figure1 ê²°ê³¼ë¡œ ë³¼ ë•Œ, Convolution Layerì˜ Output Feature-mapì€ í’ë¶€í•˜ê³  ì„œë¡œ ë¹„ìŠ·í•œ ì„±ì§ˆì„ ê°–ê³  ìˆë‹¤. ì—¬ê¸°ì— ì¤‘ì ì„ ë‘ì–´ í’ë¶€í•œ Feature-mapì˜ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ë•Œë¬¸ì— FLOPsì™€ Parameterê°€ ëŠ˜ì–´ ë‚œë‹¤ê³  ìƒê°í–ˆë‹¤
- `n` Feature-mapì— ëŒ€í•´ ê°„ë‹¨í•œ Linear ì—°ì‚°ì„ ê° Featureì— ì ìš©í•˜ì—¬ Ghost Feature ì¶”ì¶œí•œë‹¤.

$y_{ij} = Î¦_{i, j} (y'_i)    \quad âˆ€ \quad i=1,...,m, \quad j=1, ...s$

- Linear ì—°ì‚°ì¸ `Î¦` ê°ê°ì˜ Channelì— ëŒ€í•´ ë™ì‘í•˜ë©° ì¼ë°˜ì ì¸ Convolution ì—°ì‚° ë³´ë‹¤ ì—°ì‚°ì´ ê°„ë‹¨í•˜ë‹¤.

**Difference from Existing Methods**

â…°1 x 1 Pointwise Convolution ì‚¬ìš©í•˜ëŠ” ê°œì²´ë“¤ê³¼ëŠ” ë‹¬ë¦¬ Ghost Moduleì€ `Kernel_size`ì— ëŒ€í•´ `Customizing` í•  ìˆ˜ ìˆë‹¤.

â…±Ghost Moduleì€ ì¼ë°˜ì ì¸ Convolution ì‚¬ìš©í•˜ì—¬ ë¨¼ì € ëª‡ ê°œì˜ ê³ ìœ í•œ Feature-map ìƒì„±í•˜ê³ , ê°€ë²¼ìš´ `Linear Operations` í™œìš©í•˜ì—¬ Feature í™•ì¥í•˜ê³  Channel ëŠ˜ë¦°ë‹¤.

â…² Feature-map ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì´ì „ì—ì„  Depthwise, shift ì—°ì‚°ì„ ì‚¬ìš©í•œ ë°˜ë©´, Ghost Moduleì˜ Linear Operations í° ë‹¤ì–‘ì„±ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.

â…³ `instrinsic Feature-maps` ë³´ì¡´í•˜ê¸° ìœ„í•´ `identity mapping`ì€ Ghost Moduleì˜ `Linear Transform`ê³¼ ë³‘ë ¬ë¡œ ì—°ê²°ëœë‹¤.

**Analysis on Complexities**

![Figure3](./src/3.jpg)

- ì´ë¯¸ ì¡´ì¬í•˜ëŠ” êµ¬ì¡°ì— Ghost Module ì‰½ê²Œ ì¶”ê°€í•˜ì—¬ ì—°ì‚° ë¹„ìš©ì„ ì¤„ì´ë ¤ê³  ë…¸ë ¥í–ˆë‹¤.
- $n (s-1)$ Linear Operationì€ ë‹¤ì–‘í•œ `shape`ê³¼ `parameter` ì·¨í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ CPU/GPU ê¸°ë°˜ì¸online inference ì¸¡ë©´ì—ì„œ ë°©í•´ê°€ ë  ìˆ˜ ìˆë‹¤.
- ë”°ë¼ì„œ (3 x 3, 5 x 5) ê°™ì€ sizeì¸ Linear Operation ì œì•ˆí•œë‹¤.

**2. Building Efficient CNNs**

- `Ghost bottleneck(G-bneck)` ë„ì…í•˜ì—¬ ì‘ì€ CNNs ì„¤ê³„í•œë‹¤. ì œì•ˆí•œ Ghost bottleneck 2 ê°œì˜ `Ghost Modules` ìŒ“ì•„ ì˜¬ë ¤ êµ¬ì„±í•œë‹¤.
- ì²« ë²ˆì§¸ Ghost Moduleì€ `Expansion Layer` ì‚¬ìš©í•˜ì—¬ Channel ìˆ˜ë¥¼ ëŠ˜ë¦°ë‹¤.
- ë‘ ë²ˆì§¸ Ghost Moduleì€ Channel ìˆ˜ë¥¼ ì¤„ì—¬ `Shortcut path` ë§ì¶˜ë‹¤.

![Figure4](./src/4.jpg)

**GhostNet**

- ëª¨ë“  Ghost bottlenecks `stride=1` ì ìš©í•œë‹¤. ê° stage ë§ˆì§€ë§‰ì—ë§Œ `stride=2` ì ìš©í•œë‹¤.
- `squeeze and excite(SE)` module ë˜í•œ Ghost bottleneckì˜ Residual Layerì— ì ìš©í•œë‹¤.

**Width Multiplier**

- ë¹„ë¡ ì£¼ì–´ì§„ êµ¬ì¡°ê°€ low latency í•˜ê³  accuracy ë³´ì¥í•˜ì§€ë§Œ ë” ì‘ê³  ë¹ ë¥´ë©´ì„œ ë†’ì€ ì •í™•ë„ë¥¼ ê°™ì€ êµ¬ì¡°ê°€ í•„ìš”í•  ê²½ìš°ê°€ ë°œìƒí•œë‹¤.
- ë”°ë¼ì„œ `Î±` width multiplier ë„ì…í•˜ì—¬ ê° ì±„ë„ì— ì ìš©í•˜ê³  ì „ì²´ì ì¸ Networkì˜ width ìˆ˜ì •í•œë‹¤.

> **Conclusion**
> 
- ê¸°ë³¸ì ì¸ Ghost Moduleì€ ê¸°ì¡´ Convolution Layer ìª¼ê°œì–´ ë‘ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±í•œë‹¤. ê·¸ë¦¬ê³  ëª‡ëª‡ Filterì˜ Instrinsic Featuer-maps ë°œìƒ ì‹œí‚¨ë‹¤.
- ë”°ë¼ì„œ ì œì•ˆí•œ Ghost Module ì‚¬ìš©í•˜ì—¬ GhostNet ë§Œë“¤ì—ˆê³  `State-of-the-Art` ë‹¬ì„±í–ˆë‹¤.