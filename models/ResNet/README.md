# Deep Residual Learning for Image Recognition

> **Abstract**
> 
- Network êµ¬ì¡°ë¥¼ ì´ì „ê³¼ ë‹¬ë¦¬ ê¹Šê²Œ ìŒ“ì•„ í•™ìŠµì„ ì‰½ê²Œ í•˜ë ¤ê³  `Residual Learning Framework` ë°œí‘œí•œë‹¤.
- ë‹¤ë¥¸ ê¸°ëŠ¥ë“¤ì€ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©°, Layer Inputì— `Learning Residual Functions` ì ìš©í•˜ë©´ì„œ Layerì˜ êµ¬ì¡°ë¥¼ ì¬êµ¬ì„±í•˜ëŠ”  ë°©ë²•ì´ë‹¤.
- Depth (of Representations)ëŠ” ë§ì€ Visual Recognition Tasksì—ì„œ ì¤‘ìš”í•˜ë‹¤.

<aside>
ğŸ“Œ Networkì˜ Depth ëŠ˜ë¦´ ê²½ìš°, Gradient Vanishing ë¬¸ì œê°€ ë°œìƒí•œë‹¤. ë”°ë¼ì„œ ê¹Šê²Œ ìŒ“ê¸° ìœ„í•œ ë°©í–¥ì„±ì˜ ì—°êµ¬ í† ëŒ€ê°€ ë˜ì—ˆë‹¤.

</aside>

> **Introduction**
> 
- Deep Convolutional Neural NetworksëŠ” Image Classificationì—ì„œ íšê¸°ì ì¸ íë¦„ì„ ì´ëŒì–´ì™”ë‹¤.
- Deep Networks low/mid/high ë‹¨ê³„ì˜ Feature í†µí•©í•œë‹¤. ê·¸ë¦¬ê³  Multi-Layerì˜ end-to-endì˜ ClassifierëŠ” ë„ë¦¬ í¼ì§€ê²Œ í•˜ë©°, ê° Featureì˜ "Level"ì€ Layer ê¹Šê²Œ ìŒ“ìœ¼ë©´ì„œ ë³´ê°•ë  ìˆ˜ ìˆë‹¤.
- ë”°ë¼ì„œ DepthëŠ” ì¤‘ìš”í•˜ë©°, ê²°ê³¼ë¥¼ ì´ëˆë‹¤.

![1.JPG](Deep%20Residual%20Learning%20for%20Image%20Recognition%207354b7eeff774dcf9f7f9452d889cf1a/1.jpg)

- Depthì˜ ì¤‘ìš”ì„±ì— ì…ê°í•˜ì—¬, "Networkì— Layer ì¶”ê°€í•˜ëŠ” ë°©í–¥ì´ ë” ì¢‹ì€ê°€? ì— ëŒ€í•œ ì§ˆë¬¸ì´ ë”°ë¼ì˜¤ê²Œ ëœë‹¤.
- ì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸°ì— ê°€ì¥ í° ì¥ì• ë¬¼ì€ `Vanishing/Exploding Gradients` ë¬¸ì œì´ë‹¤.
- ì´ ë¬¸ì œì— ëŒ€í•´ `normalized initialization` ë° `intermediate normalization layers` 10ê°œ ì´ìƒì˜ Layer ìˆ˜ë ´í•˜ê²Œ ë§Œë“¤ì—ˆë‹¤.
- ê·¸ëŸ¬ë‚˜ Networkê°€ ê¹Šì–´ì§€ë©´ ì •í™•ë„ í•˜ë½ ë¬¸ì œê°€ ë°œìƒí•˜ì˜€ë‹¤. ì´ í•˜ë½ì€ Overfitting ì¸í•´ ë°œìƒëœ ê²ƒì€ ì•„ë‹ˆë‹¤. ì¦‰, Deep Modelì´ Shallower Modelì— ë¹„í•´ ë†’ì€ Training Error ë°œìƒì‹œì¼°ë‹¤.
- í•´ë‹¹ ë…¼ë¬¸ì—ì„œ Layer ì§ì ‘ì ìœ¼ë¡œ ìŒ“ì§€ ì•Šê³ , `Residual Mapping` ì´ìš©í•œë‹¤.

$F(x) := H(x) - x$

![2.JPG](Deep%20Residual%20Learning%20for%20Image%20Recognition%207354b7eeff774dcf9f7f9452d889cf1a/2.jpg)

- í•´ë‹¹ ê³µì‹ì˜ ê²½ìš° `Shortcut Conntection` ì´í•´í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” í•˜ë‚˜ ë˜ëŠ” ì—¬ëŸ¬ Layer ê±´ë„ˆ ë›´ë‹¤.
- ì €ìë“¤ì€ `Identiy Mapping` ì´ìš©í•˜ì—¬ ìˆ˜í–‰í•˜ì˜€ê³ , ì´ outputì€ ìŒ“ì—¬ì§„ Layerì— ë”í•´ì§„ë‹¤. `Identiy Shortcut Connection` ì¶”ê°€ Parameterê°€ í•„ìš”í•˜ì§€ ì•Šê³ , Computation ë˜í•œ ë³µì¡í•˜ì§€ ì•Šë‹¤.

<aside>
ğŸ“Œ **[ResNet ì‹¤í—˜ ê²°ê³¼]**
1. Depthê°€ ì¦ê°€í•  ìˆ˜ë¡ ë‹¤ë¥¸ NetworkëŠ” Training Errorê°€ ë†’ì€ ë°˜ë©´ Deep Residual Netsì€ ê·¸ë ‡ì§€ ì•Šë‹¤.
2. Depthê°€ ì¦ê°€ í•´ë„ ì´ì „ Networkì— ë¹„í•´ ë” ë†’ì€ ì •í™•ë„ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

</aside>

> **Deep Residual Learning**
> 

**Residual Learning**

- $H(x)$ì—ì„œ xëŠ” ì´ëŸ¬í•œ Layerì˜ Input ì§€ì¹­í•œë‹¤. ë”°ë¼ì„œ H(x)ì— ìœ ì‚¬í•˜ê²Œ ë§Œë“œëŠ” ëŒ€ì‹  F(x) := H(x) - xì— ê·¼ì ‘ ì‹œí‚¤ë„ë¡ í•œë‹¤. ë”°ë¼ì„œ F(x) + xê°€ ëœë‹¤. ë‘ í˜•íƒœ ëª¨ë‘ ì¶”ì •í•  ìˆ˜ ìˆì–´ì•¼ í•˜ì§€ë§Œ, í•™ìŠµì˜ ìš©ì´ì„±ì€ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤.
- ë§Œì•½ ì¶”ê°€ Layerê°€ `Identiy Mappings` ì´ìš©í•˜ì—¬ ì„¤ê³„ëœë‹¤ë©´, Deep Modelì€ Shallower Model ë³´ë‹¤  Training Errorê°€ ë‚®ì„ ê²ƒì´ë‹¤.
- `Optimal Function` ê°€ Zero Mapping ë³´ë‹¤ Identity Mappingì— ê°€ê¹Œìš¸ ê²½ìš°, SolverëŠ” ìƒˆë¡œìš´ í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒë³´ë‹¤ Identity Mapping ìˆ˜ì •í•˜ëŠ” ê²ƒì´ ë” ì‰¬ì›Œì•¼ í•œë‹¤.

 **Identity Mapping by Shorcuts**

- $y = F(x, [w^i]) + x$ Building Block ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆë‹¤. í•´ë‹¹ ìˆ˜ì‹ì—ì„œ xì™€ yëŠ” input ë° output ì§€ì¹­í•˜ê³ , F(x)ëŠ” Residual Mapping í•™ìŠµ ìˆ˜ì‹ì„ ì˜ë¯¸í•œë‹¤.
- $F = W2âˆ‚(W1x)$ âˆ‚ëŠ” `ReLU` ì˜ë¯¸í•˜ê³ , `bias`ëŠ” ìƒëµëœë‹¤.
- F + x ì—°ì‚°ì€ `Element - Wise Addition` ì§„í–‰ëœë‹¤.
- Fì™€ xì˜ `Dimensions`ì´ ë‹¤ë¥¸ ê²½ìš° `Linear Projection` í†µí•´ Shortcut Connection ì§„í–‰í•œë‹¤.

<aside>
ğŸ“Œ Element-wise Addition 2ê°œì˜ Feature mapì—ì„œ Channel by Channel ì§„í–‰

</aside>

**Network Architectures**

![4.JPG](Deep%20Residual%20Learning%20for%20Image%20Recognition%207354b7eeff774dcf9f7f9452d889cf1a/4.jpg)

![3.JPG](Deep%20Residual%20Learning%20for%20Image%20Recognition%207354b7eeff774dcf9f7f9452d889cf1a/3.jpg)

- ResNet (Residual Network) êµ¬ì„±í•˜ë©´ì„œ 2ê°€ì§€ Option ê³ ë ¤í•œë‹¤.
1. Shortcutì€ zero padded í•˜ì—¬ ì°¨ì›ì„ ì¦ê°€ ì‹œí‚¤ë©° ìˆ˜í–‰ëœë‹¤. (Parameter ì¦ê°€ ì—†ìŒ)
2. Dimension Maching í•˜ê¸° ìœ„í•´ Projection ì‚¬ìš©í•œë‹¤. (1 x 1 convolution)

<aside>
ğŸ“Œ Skip Connection êµ¬í˜„í•  ê²½ìš°, Stride = 2ì— ëŒ€í•œ ë¶€ë¶„ì— ëŒ€í•´ ëª…ì‹œê°€ ë˜ì–´ ìˆì§€ ì•Šë‹¤.
ë˜í•œ input â†’ 1 x 1 Convolution ì§„í–‰í•  ë•Œ, Stride = 2 ì ìš©í•˜ì.

</aside>

> **Experiments**
> 

**Deeper Bottlenect Architectures**

![5.JPG](Deep%20Residual%20Learning%20for%20Image%20Recognition%207354b7eeff774dcf9f7f9452d889cf1a/5.jpg)

- Training-Time ì¸í•˜ì—¬ Building Block â†’ `BottleNeck` ë””ìì¸í•˜ì˜€ë‹¤.
- í•´ë‹¹ `1 x 1 Convolution` ì°¨ì›ì„ ì¤„ì´ê³  ëŠ˜ë¦¬ëŠ” ê¸°ëŠ¥ë§Œ ë‹´ë‹¹í•œë‹¤. ì¼ë°˜ 3 x 3 ì—°ì‚°ê³¼ ë™ì¼í•œ Complexity ê°–ëŠ”ë‹¤.
- BottleNect ì´ìš©í•˜ì—¬ Identity Shortcut íš¨ê³¼ì ì¸ ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤.