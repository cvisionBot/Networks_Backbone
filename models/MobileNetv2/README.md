# MobileNetV2: Inverted Residuals and Linear Bottlenecks

> **Abstract**
> 
- ìƒˆë¡œìš´ Mobile Architecture, **MobileNetV2** ì œì‹œí•œë‹¤.
- `Object Detection` ì§„í–‰í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ frameworkì¸ `MobileDeepLabv3` ì‚¬ìš©í•˜ì—¬ Mobile Model íš¨ìœ¨ì ì¸ ë°©ë²•ìœ¼ë¡œ ì œì‹œí•œë‹¤.
- `Inverted Residual` êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬, `bottleneck` êµ¬ê°„ì— `Shortcut Connections` ì§„í–‰í•œë‹¤.
- ì¤‘ê°„ Expansion Layer ëŒ€í•´ ê°€ë²¼ìš´ `Depthwise Convolution` ì ìš©í•´ non-lineartiyí•œ Feauture ì¶”ì¶œí•œë‹¤.
- ê·¸ë¦¬ê³  narrow layerì—ì„œ non-lineartiy ì œê±°í•˜ëŠ” ê²ƒì´ Feature ì˜ ìœ ì§€í•  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆë‹¤.
- ë…¼ë¬¸ì˜ ì ‘ê·¼ ë°©ì‹ì€ Input/Output ë„ë©”ì¸ ë¶„ë¦¬ë¥¼ í—ˆìš©í•˜ê¸° ë•Œë¬¸ì— ë¶„ì„ì„ ìœ„í•œ frameworkì—ì„œ í¸ë¦¬í•˜ê²Œ ì‘ë™í•œë‹¤.

> **Introduction**
> 
- Networkì˜ ì •í™•ë„ í–¥ìƒì„ ìœ„í•´ì„  í•­ìƒ ë¹„ìš© ì¸¡ë©´ì´ ë”°ë¼ì˜¨ë‹¤ : í˜„ì¬ `state of the art` NetworkëŠ” Mobile ë° Embedded Applicationì˜ ìˆ˜ìš©ì„±ì„ ë„˜ì–´ ë†’ì€ ì—°ì‚°ì„ ìš”êµ¬í•œë‹¤.
- ë”°ë¼ì„œ ë…¼ë¬¸ì€ ìƒˆë¡œìš´ Neural Network êµ¬ì¡°ë¥¼ í†µí•´ Mobile ë° Resource ì œí•œ í™˜ê²½ì— ëŒ€í•œ ë§ì¶¤í˜• í•´ê²°ì„ ì œì•ˆí•œë‹¤.
- ì €ìë“¤ì€ ìƒˆë¡œìš´ Layer Moduleì— ë§ì€ ì¤‘ì ì„ ë‘ì—ˆë‹¤ : `Linear Bottleneck` ì‚¬ìš©í•˜ëŠ” `Inverted Residual Block`ì´ë‹¤.
- ì´ Moduleì€ `Low-Dimensional Feature` ì…ë ¥ìœ¼ë¡œ ë°›ì•„ `High-Dimension` í™•ì¥í•˜ê³ , Depthwise Convolution ì´ìš©í•˜ì—¬ Filtering ì‘ì—…ì„ ì§„í–‰í•œë‹¤. ì´ í›„, FeatureëŠ” Linear-Convolution ì‚¬ìš©í•´ ë‹¤ì‹œ `Low-Dimension` íˆ¬ì˜ëœë‹¤.
- ì´ëŸ¬í•œ Convolution Moduleì€ íŠ¹íˆ Mobile Designì— ì í•©í•˜ë‹¤. ì™œëƒí•˜ë©´ Tensor ì¤‘ê°„ ë‹¨ê³„ë¡œ ë³µì›í•˜ëŠ” ê³¼ì •ì´ í•„ìš” ì—†ê¸° ë•Œë¬¸ì— Memory Footprint ì´ì ì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤.

> **Preliminaries, discussion and intuition**
> 

**Depthwise Separable Convolutions**

- `Depthwise Separable Convolutions Block`ì€ ë§ì€ íš¨ìœ¨ì ì¸ Neural Network êµ¬ì¡°ì—ì„œ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë©°, ë…¼ë¬¸ì˜ ì €ìë“¤ ë˜í•œ ì‚¬ìš©í•œë‹¤.
- ì²« ë²ˆì§¸ LayerëŠ” `Depthwise Convolution` ì§„í–‰í•˜ë©°, Input Channelì— ëŒ€í•´ ë‹¨ì¼ Convolution Filtering ìˆ˜í–‰í•œë‹¤.
- ë‘ ë²ˆì§¸ LayerëŠ”  1 x 1 Convolution ì§„í–‰í•˜ëŠ” `Pointwise Convolution`ì´ë‹¤. í•´ë‹¹ LayerëŠ” Input Channelsì— ëŒ€í•´ Linear Combination ì—°ì‚°í•˜ì—¬ ìƒˆë¡œìš´ Features ì¶”ì¶œí•œë‹¤.
- `Depthwise Separable Convolution`ì€ ë‹¤ë¥¸ Layer ë¹„í•´ ì—°ì‚°ì„ ì¤„ì¼ ìˆ˜ ìˆìœ¼ë©°, **MobileNetV2**ì—ì„œ `k = 3` â†’ (3 x 3 Depthwise Separable Convolution)ì´ë‹¤.

**Linear Bottlenecks**

- Real Imageì¸ Input Set ê°–ê³ , Layer Activation í†µê³¼í•˜ë©´ **"Maniford of Interest"** í˜•ì„±ëœë‹¤ëŠ” ê²ƒì„ ì•ˆë‹¤. ê·¸ë¦¬ê³  ì´ë ‡ê²Œ í˜•ì„±ëœ FeatureëŠ” Neural Networkì—ì„œ `Low-Dimensional Subspace`ì— ë“¤ì–´ê°€ê²Œ ëœë‹¤.
- ì´ëŸ¬í•œ ì‚¬ì‹¤ì€ Layerì˜ Dimensionality ì¤„ì´ê±°ë‚˜, Operating Spaceì˜ Dimensionality ì¤„ì´ë©´ ì•Œ ìˆ˜ ìˆë‹¤.
- **MobileNetV1** ì§ê´€ì„ ë”°ë¥´ë©´ì„œ, `width_multiplier` ì ‘ê·¼ì€ Featuresê°€ ì „ì²´ ê³µê°„ì— ë“¤ì–´ê°€ê¸°ê¹Œì§€ Activation Spaceì˜ Dimensionality ì¤„ì´ëŠ” ê²ƒì„ í—ˆë½í•œë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ì§ê´€ì€ Deep Convolutional Neural Networkê°€ `ReLU`ì™€ ê°™ì€ **í™œì„±í™” í•¨ìˆ˜(non-linear)** ì‚¬ìš©í•˜ëŠ” ìƒê°ì„ í•œë‹¤ë©´ ì˜¤ë¥˜ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
- Deep NetworkëŠ” `Non-Zero`ì¸ Linear Classifier Output ë„ë©”ì¸ë§Œ ê°–ê¸° ë•Œë¬¸ì´ë‹¤.

![Figure1](./src/1.jpg)

- ë‹¤ì‹œ ë§í•´ `ReLU` í†µê³¼í•  ë•Œ, ë¶ˆê°€í”¼í•˜ê²Œ ì •ë³´ê°€ ì†ì‹¤ ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë§í•œë‹¤. ê·¸ëŸ¬ë‚˜ ë§Œì•½ Channel ê°œìˆ˜ê°€ ë§ë‹¤ë©´ Activation í†µê³¼í•˜ë”ë¼ë„ Manifold ì •ë³´ëŠ” ë‹¤ë¥¸ Channelì— ë‚¨ì•„ ìˆì„ ìˆ˜ ìˆë‹¤.
- ìš”ì•½ í•˜ìë©´, Manifold of InterestëŠ” `Higher-Dimensional Activation Space`ì˜ `Low-Dimensional Subspace`ì— ë†“ì—¬ ìˆì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.

<aside>
ğŸ“Œ **[Tow Properties]**
1. Manifold of InterestëŠ” `ReLU` í†µê³¼í•˜ë”ë¼ë„ linear Transformationê³¼ ìƒì‘í•  ìˆ˜ ìˆê²Œ, `None-Zero` ë‚¨ì•„ìˆì–´ì•¼ í•œë‹¤.
2. `ReLU`ëŠ” Input Manifold ì™„ë²½í•˜ê²Œ ë³´ì¡´ ë¿ë§Œ ì•„ë‹ˆë¼ Input ManifoldëŠ” `Low-Dimensional Subspace`ì— ë†“ì—¬ ìˆì–´ì•¼ í•œë‹¤.

</aside>

- ìœ„ ë‘ ê°€ì§€ ê´€ì ì´ Neural Architectrues ì„¤ê³„í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤.

**Inverted Residuals**

![Figure2](./src/2.jpg)

- Bottleneck Blocksì€ Residual Blockê³¼ ë¹„ìŠ·í•˜ê²Œ ë³´ì¸ë‹¤. ê° Blockì€ Input í¬í•¨í•˜ê³  ìˆìœ¼ë©° ì—¬ëŸ¬ Bottlenecksê³¼ Expansionì´ ë’¤ë”°ë¥¸ë‹¤.
- `Shortcuts` ì‚½ì…í•˜ëŠ” ì´ìœ ëŠ” ê³ ì „ì ì¸ Residual Connectionsê³¼ ì˜ë¯¸ê°€ ë¹„ìŠ·í•˜ë‹¤. ì´ë¡œ ì¸í•´ `Gradient Propagate` ëŠ¥ë ¥ì„ í–¥ìƒ ì‹œí‚¤ê¸° ìœ„í•¨ì´ë‹¤.
- Inverted Design ìƒë‹¹íˆ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ë©° ë” ì˜ ì‘ë™ëœë‹¤.

**Inverted Residual - Running Time and Parameter Count for Bottleneck Convolution**

![Figure3](./src/3.jpg)

<aside>
ğŸ“Œ [Inverted Residual Block Params]
`h`, `w` : block sizes
`t` : expansion facotr
`k` : kernel_size
`d'` : input_channels
`d''` : output_channels

</aside>

- Total number of multiply add  : $h * w * d' * t(d' + k^2 + d'')$

**Information Flow Interpretation**

- êµ¬ì¡°ì˜ ì¥ì ì€ Building Blockì´ Input/Output ë„ë©”ì¸ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„ë¦¬ ì‹œí‚¨ë‹¤ëŠ” ê²ƒì´ë‹¤.
- ê·¸ë¦¬ê³  ê¸°ì¡´ê³¼ ë‹¤ë¥´ê²Œ `Expansion Ratio` 1 ë³´ë‹¤ í¬ê²Œ í•˜ì—¬ ìœ ìš©í•˜ê²Œ í•œë‹¤.

> **Model Architecture**
> 

![Figure4](./src/4.jpg)

- Non-linearity â†’ `ReLU6` ì‚¬ìš©í•˜ì˜€ë‹¤. ì™œëƒí•˜ë©´ í•´ë‹¹ í™œì„±í™” í•¨ìˆ˜ëŠ” Low-precision Computationì— ê°•ì¸í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë‹¤ìŒ ëª¨ë“  `3 x 3 kernel_size` ì‚¬ìš©í•˜ê³ , `BatchNorm` ì—°ì‚°ì„ ì¶”ê°€í•œë‹¤.

**Trade-off Hyper Parameters**

- ì €ìë“¤ì€ 96 ~ 224 Resolutionì— ëŒ€í•´ ì‹¤í—˜í•˜ë©°, `width_multiplier` (0.35 to 1.4)ê°€ ìµœì ì¸ ê²ƒì„ ì•Œì•„ë‚´ì—ˆë‹¤.

> **Conclusions and future work**
> 
- ì €ìë“¤ì€ ë†’ì€ íš¨ìœ¨ì„±ì„ ê°–ëŠ” Mobile Model ì œì•ˆí•˜ì˜€ë‹¤. Inference ì¸¡ë©´ì—ì„œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì´ ìˆë‹¤.
- **Theoretical Side :** Inverted Residual Blockì€ Expansion Layerì˜ `Expressiveness`ì™€ Bottlenec Inputì˜ `Capacity` ë¶„ë¦¬í•  ìˆ˜ ìˆëŠ” ì†ì„±ì„ ê°–ê³  ìˆë‹¤.